{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Task 01 - openAI API\n",
        "### Erick Badillo"
      ],
      "metadata": {
        "id": "BrZ6RumuJeC8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tda7Uk1O0-1E"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('openai-secret')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDaRS2UwAWSd",
        "outputId": "50a9c60d-5483-4ef5-8201-af71ea962e1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.11.1-py3-none-any.whl (226 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/226.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m225.3/226.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.1/226.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.14)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typing-extensions, h11, httpcore, httpx, openai\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 openai-1.11.1 typing-extensions-4.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=api_key)"
      ],
      "metadata": {
        "id": "l4NHb5mBAx8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(text):\n",
        "    \"\"\"\n",
        "    Function to summarize a text using GPT 3.5 Turbo\n",
        "\n",
        "    :param text: The text to be summarized.\n",
        "    :return: The summarized text.\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful AI assistant. Summarize the following text.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": text\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "5HOEVcWrA6n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare(text1, text2):\n",
        "  \"\"\"\n",
        "  Function to compare text using GPT 3.5 Turbo\n",
        "\n",
        "  :param text1: Text to compare\n",
        "  :param text2: Text to compare\n",
        "  :return: The comparison result\n",
        "  \"\"\"\n",
        "  response = client.chat.completions.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant. Compare the following two texts.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Please compare these two texts:\\n\\nText 1: {text1}\\n\\nText 2: {text2}\"}\n",
        "        ]\n",
        "  )\n",
        "\n",
        "  return response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "ZUTaWj0LGkoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_abstract = \"\"\"\n",
        "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "p3HIvFad5QW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_abstract = \"\"\"\n",
        "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Mrf1InOnF3j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary 1"
      ],
      "metadata": {
        "id": "zkrho-NEJPO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_abstract_summary = summarize(rag_abstract)\n",
        "print(rag_abstract_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vaee02vx6gQF",
        "outputId": "85f710c6-cd45-43cd-9cd3-9492925c7b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text discusses the limitations of large pre-trained language models in accessing and manipulating knowledge, especially on knowledge-intensive tasks. The authors propose a solution by combining pre-trained parametric and non-parametric memory in retrieval-augmented generation (RAG) models. They specifically explore RAG models with a pre-trained seq2seq model as parametric memory and a dense vector index of Wikipedia as non-parametric memory accessed by a pre-trained neural retriever. The authors compare two formulations of RAG models and fine-tune them on various knowledge-intensive NLP tasks. The results show that RAG models outperform parametric seq2seq models and task-specific retrieve-and-extract architectures, achieving state-of-the-art performance on three open domain QA tasks. Additionally, RAG models generate more specific, diverse, and factual language compared to a state-of-the-art parametric-only seq2seq model for language generation tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary 2"
      ],
      "metadata": {
        "id": "PU9T6_xUJSh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_abstract_summary = summarize(lora_abstract)\n",
        "print(lora_abstract_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIHBrjOuGCIg",
        "outputId": "f074b588-13c3-437d-a526-542ea07d00c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text discusses a new technique called Low-Rank Adaptation (LoRA) for natural language processing. It addresses the challenge of large-scale pre-training and fine-tuning of models, which can be expensive and memory-intensive. LoRA freezes pre-trained model weights and adds trainable rank decomposition matrices to each layer of the model, drastically reducing the number of trainable parameters. Despite having fewer parameters, LoRA performs as well as or better than traditional fine-tuning methods on various language models. The text also mentions that the authors have released a package that integrates LoRA with PyTorch models and provides implementations and model checkpoints for several models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summaries comparison"
      ],
      "metadata": {
        "id": "iynn-5HEJWhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comparison_result = compare(rag_abstract_summary, lora_abstract_summary)\n",
        "print(comparison_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP2jBDXLGbcn",
        "outputId": "c2842930-1011-443d-bef3-fe38a67d2204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1 discusses the limitations of large pre-trained language models on knowledge-intensive tasks and proposes a solution called retrieval-augmented generation (RAG) models. It explores RAG models with a combination of pre-trained parametric and non-parametric memory to improve access and manipulation of knowledge. The authors compare different formulations of RAG models and fine-tune them on various NLP tasks, showing that RAG models outperform other models and achieve state-of-the-art performance on open domain QA tasks. They also highlight that RAG models generate more specific, diverse, and factual language compared to other models for language generation tasks.\n",
            "\n",
            "Text 2 introduces a technique called Low-Rank Adaptation (LoRA) for addressing the challenge of large-scale pre-training and fine-tuning of models in natural language processing. LoRA freezes pre-trained model weights and adds trainable rank decomposition matrices to reduce the number of trainable parameters. Despite having fewer parameters, LoRA performs as well as or better than traditional fine-tuning methods on different language models. The text also mentions that the authors have released a package that integrates LoRA with PyTorch models and provides implementations and model checkpoints for multiple models.\n"
          ]
        }
      ]
    }
  ]
}